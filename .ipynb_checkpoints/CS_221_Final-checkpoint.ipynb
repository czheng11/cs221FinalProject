{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcbsnDYk3AV9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Packages to import\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l1, l2\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "import operator\n",
    "from math import log\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn import datasets\n",
    "\n",
    "#Models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! \n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Icy10EFN3IXB"
   },
   "outputs": [],
   "source": [
    "def loadData(who):\n",
    "    all_filenames = ['2014_clean1.csv', '2016_clean1.csv', '2017_clean1.csv', '2018_clean1.csv']\n",
    "    #combine all files in the list\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "    #export to csv\n",
    "    combined_csv.to_csv(\"combined.csv\", index=False, encoding='utf-8-sig')\n",
    "    train=pd.read_csv('2018_clean1.csv')\n",
    "    combined=pd.read_csv('combined.csv')\n",
    "    colName = 'discuss_mental_'+who+'_Maybe'\n",
    "    if colName in combined:\n",
    "        combined = combined[combined[colName] == 0]\n",
    "    else: print('malformed')\n",
    "    return train, combined\n",
    "\n",
    "def performLogReg(X,y, test_sz = 0.2, random_state = 42):\n",
    "  # split X and y into training and testing sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sz, random_state=random_state)\n",
    "\n",
    "  # instantiate the model (using the default parameters)\n",
    "  logreg = LogisticRegression()\n",
    "\n",
    "  # fit the model with data\n",
    "  logreg.fit(X_train,y_train)\n",
    "\n",
    "  # predict\n",
    "  y_pred=logreg.predict(X_test)\n",
    "\n",
    "  print(classification_report(y_test,y_pred))\n",
    "\n",
    "  cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "  print(\"Confusion matrix: \\n\" + str(cnf_matrix))\n",
    "  print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "  print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "  print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "  y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "  fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "  plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "  plt.legend(loc=4)\n",
    "  plt.title(y.columns[0])\n",
    "  plt.close()\n",
    "  return logreg\n",
    "\n",
    "def printAndShowCoef(feature_cols, model):\n",
    "  weights_dict = {}\n",
    "  for i, feature in enumerate(feature_cols):\n",
    "    weights_dict[feature] = model.coef_[0][i]\n",
    "  weights_dict = sorted(weights_dict.items(), key=operator.itemgetter(1))\n",
    "  \n",
    "  top_10 = weights_dict[-10:]\n",
    "  top_10.reverse()\n",
    "  zip(*top_10)\n",
    "  plt.figure(figsize=(15,5))\n",
    "  plt.scatter(*zip(*top_10))\n",
    "  plt.title(y.columns[0] + \" top positive features\")\n",
    "  plt.savefig(y.columns[0] + ' top_pos6.png')\n",
    "  plt.close()\n",
    "\n",
    "  low_10 = weights_dict[:10]\n",
    "  zip(*low_10)\n",
    "  plt.figure(figsize=(15,5))\n",
    "  plt.scatter(*zip(*low_10))\n",
    "  plt.title(y.columns[0] + \" top negative features\")\n",
    "  plt.savefig(y.columns[0] + ' neg_6.png')\n",
    "  return weights_dict\n",
    "\n",
    "def buildData(dataset, who, train):\n",
    "  #split dataset in features and target variable\n",
    "  # feature_cols = ['country', 'state', 'self_employed', 'family_history', 'treatment', 'interfere', 'size', 'tech',\t'benefits',\t'options', 'wellness_program',\t'resources', 'anon', 'leave', 'interview_mental_health', 'interview_physical_health', 'observe_negative']\n",
    "  root = 'discuss_mental_'+who \n",
    "  target_col = root+'_Yes'\n",
    "  ignoreAdd = [root+'_Maybe', root+'_No', target_col, root+'_Some of them']\n",
    "  if who != 'supervisor':\n",
    "        ignoreAdd.extend(['Unnamed: 0', root+'nan'])\n",
    "  feature_cols = list(train.columns)\n",
    "  remove_list = ['age', 'country', 'state', 'work_remotely','mental_health_seriously', 'employer_negative_consequence_mental', 'employer_negative_consequence_physical']\n",
    "  remove_list.extend(ignoreAdd)\n",
    "  for delete_info in remove_list:\n",
    "    if delete_info in train.columns:\n",
    "      feature_cols.remove(delete_info)\n",
    "  X = dataset[feature_cols] # Features\n",
    "  for feature in feature_cols:\n",
    "    X.loc[X[feature] != 1, feature] = 0\n",
    "  y = dataset[[target_col]] # Target variable\n",
    "  # for feature in feature_cols:\n",
    "  y.to_csv(\"y.csv\", index=False, encoding='utf-8-sig')\n",
    "  y.loc[y[target_col] != 1, target_col] = 0\n",
    "  return X, y, feature_cols\n",
    "  return(performLogReg(X,y, feature_cols))\n",
    "\n",
    "def printListCoefs(X, logreg):\n",
    "    features = list(X.columns)\n",
    "    coefficients = {}\n",
    "    for i, f in enumerate(features):\n",
    "      coefficients[f] = logreg.coef_[0][i]\n",
    "    import operator\n",
    "    sorted_coefficients = sorted(coefficients.items(), key=operator.itemgetter(1))\n",
    "    print(sorted_coefficients[::-1])\n",
    "    print('Number of features: ', len(features))\n",
    "    print('Number of entries: ',len(X))\n",
    "\n",
    "def runDifferentModels(X,Y):\n",
    "  # split X and y into training and testing sets\n",
    "  X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.3)\n",
    "  X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "  print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "  #\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\"Naive Bayes\", \"QDA\"\n",
    "  models = []\n",
    "  models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "  models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "  models.append(('GP', GaussianProcessClassifier()))\n",
    "  models.append(('RF', RandomForestClassifier()))\n",
    "  models.append(('NN', MLPClassifier()))\n",
    "  models.append(('AB', AdaBoostClassifier()))\n",
    "  models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
    "  models.append(('CART', DecisionTreeClassifier()))\n",
    "  models.append(('NB', GaussianNB()))\n",
    "  models.append(('K_M', KNeighborsClassifier(metric = 'minkowski')))\n",
    "  models.append(('K_C', KNeighborsClassifier(metric = 'chebyshev')))\n",
    "  models.append(('K_E', KNeighborsClassifier(metric = 'euclidean')))\n",
    "  models.append(('K_Man', KNeighborsClassifier(metric = 'manhattan')))\n",
    "  models.append(('K_Mat', KNeighborsClassifier(metric = 'matching')))\n",
    "  models.append(('K_J', KNeighborsClassifier(metric = 'jaccard')))\n",
    "  models.append(('K_D', KNeighborsClassifier(metric = 'dice')))\n",
    "  models.append(('K_K', KNeighborsClassifier(metric = 'kulsinski')))\n",
    "  models.append(('S_Lin', SVC(kernel='linear')))\n",
    "  models.append(('S_RBF', SVC(kernel='rbf')))\n",
    "  models.append(('S_Sig', SVC(kernel=\"sigmoid\")))\n",
    "  models.append(('S_Pol', SVC(kernel=\"poly\")))\n",
    "  \n",
    "\n",
    "  # Evaluate each model through 10-fold cross-validation\n",
    "  results = []\n",
    "  names = []\n",
    "  number = 1\n",
    "  for name, model in models:\n",
    "      kfold = KFold(n_splits=10, random_state=42)\n",
    "      cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "      results.append(cv_results)\n",
    "      names.append(name)\n",
    "      msg = \"%i) %s: Mean = %f with std = (%f)\" % (number, name, cv_results.mean(), cv_results.std())\n",
    "      number += 1\n",
    "      print(msg)\n",
    "  # Compare Algorithms\n",
    "  matplotlib.rcParams.update({'font.size': 20})\n",
    "  fig = plt.figure()\n",
    "  fig = plt.figure(figsize=(14,10))\n",
    "  plt.title('Algorithm Comparison')\n",
    "  ax = fig\n",
    "  medianprops = {'color': 'magenta', 'linewidth': 3}\n",
    "  boxprops = {'color': 'black', 'linewidth': 2, 'linestyle': '-'}\n",
    "  whiskerprops = {'color': 'black', 'linewidth': 2, 'linestyle': '-'}\n",
    "  capprops = {'color': 'black', 'linewidth': 2, 'linestyle': '-'}\n",
    "  flierprops = {'color': 'black', 'marker': 'x'}\n",
    "  plt.boxplot(results,\n",
    "           medianprops=medianprops,\n",
    "           boxprops=boxprops,\n",
    "           whiskerprops=whiskerprops,\n",
    "           capprops=capprops,\n",
    "           flierprops=flierprops)\n",
    "#   ax.set_xticklabels(names)\n",
    "  plt.xlabel('Model Number', fontsize=20)\n",
    "  plt.ylabel('Accuracy', fontsize=20)\n",
    "  plt.savefig('Algorithm Comparison')\n",
    "\n",
    "def runKNN_SVMModels(X,y):\n",
    "  # split X and y into training and testing sets\n",
    "  X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.3)\n",
    "  X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "  print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n",
    "  #\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\"Naive Bayes\", \"QDA\"\n",
    "  models = []\n",
    "  models.append(('K_M', KNeighborsClassifier(metric = 'minkowski')))\n",
    "  models.append(('K_C', KNeighborsClassifier(metric = 'chebyshev')))\n",
    "  models.append(('K_E', KNeighborsClassifier(metric = 'euclidean')))\n",
    "  models.append(('K_Man', KNeighborsClassifier(metric = 'manhattan')))\n",
    "  models.append(('K_Mat', KNeighborsClassifier(metric = 'matching')))\n",
    "  models.append(('K_J', KNeighborsClassifier(metric = 'jaccard')))\n",
    "  models.append(('K_D', KNeighborsClassifier(metric = 'dice')))\n",
    "  models.append(('K_K', KNeighborsClassifier(metric = 'kulsinski')))\n",
    "  models.append(('S_Lin', SVC(kernel='linear')))\n",
    "  models.append(('S_RBF', SVC(kernel='rbf')))\n",
    "  models.append(('S_Sig', SVC(kernel=\"sigmoid\")))\n",
    "  models.append(('S_Pol', SVC(kernel=\"poly\")))\n",
    "  \n",
    "  # Evaluate each model through 10-fold cross-validation\n",
    "  results = []\n",
    "  names = []\n",
    "  for name, model in models:\n",
    "      kfold = KFold(n_splits=10, random_state=42)\n",
    "      cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "      results.append(cv_results)\n",
    "      names.append(name)\n",
    "      msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "      print(msg)\n",
    "  # Compare Algorithms\n",
    "  fig = plt.figure()\n",
    "  \n",
    "  fig.suptitle('Algorithm Comparison')\n",
    "  ax = fig.add_subplot(111)\n",
    "  plt.boxplot(results)\n",
    "  ax.set_xticklabels(names)\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.savefig('Comparison.png')\n",
    "\n",
    "def testPCA(X, Y):\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "                 , columns = ['principal component 1', 'principal component 2'])\n",
    "    fig = plt.figure(figsize = (14,10))\n",
    "    # ax = Axes3D(fig)\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 25)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 25)\n",
    "    ax.set_title('2 component PCA', fontsize = 30)\n",
    "    targets = ['discuss_mental_supervisor_Yes']\n",
    "    colors = ['r']\n",
    "    for target, color in zip(targets,colors):\n",
    "        cset = ax.scatter(principalDf['principal component 1']\n",
    "                   , principalDf['principal component 2']\n",
    "                   , 16)\n",
    "    plt.savefig('PCA2.png')\n",
    "    print(principalDf.columns)\n",
    "    print(len(X.columns))\n",
    "    print(Y)\n",
    "\n",
    "    performLogReg(principalDf,Y)\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "                 , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "    print(principalDf.columns)\n",
    "    print(len(X.columns))\n",
    "    print(Y)\n",
    "\n",
    "    performLogReg(principalDf,Y)\n",
    "\n",
    "    pca = PCA().fit(X)\n",
    "    #Plotting the Cumulative Summation of the Explained Variance\n",
    "    fig = plt.figure(figsize = (14,10))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_), )\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance (%)') #for each component\n",
    "    plt.title('Dataset Explained Variance')\n",
    "    plt.savefig('PCAVariance.png')\n",
    "\n",
    "    my_model = PCA(n_components=0.99, svd_solver='full')\n",
    "    my_model.fit_transform(X)\n",
    "    print(my_model.explained_variance_ratio_)\n",
    "    print(len(my_model.explained_variance_ratio_))\n",
    "    print(my_model.explained_variance_ratio_.cumsum())\n",
    "\n",
    "def runNeuralNet(X, Y, test_size = 0.3):\n",
    "    # X_train (10 input features, 70% of full dataset), X_val (10 input features, 15% of full dataset), X_test (10 input features, 15% of full dataset)\n",
    "    # Y_train (1 label, 70% of full dataset), Y_val (1 label, 15% of full dataset), Y_test (1 label, 15% of full dataset)\n",
    "    X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=test_size)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "    print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n",
    "\n",
    "    # Hyperparameter turning\n",
    "    # values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "    # for param in values:\n",
    "    #     # model = Sequential([Dense(32, activation='relu', input_shape=(58,)), Dense(32, activation='relu'), Dense(1, activation='sigmoid'),]) # described sequentially, layer-by-layer with 32, 32, and 1 neurons; 58 input features\n",
    "    #     print(\"Param = \" + str(param))\n",
    "        \n",
    "    # SGD Optimizer\n",
    "    param = 1e-2\n",
    "    model = Sequential([Dense(32, activation='relu', input_shape=(58,), kernel_regularizer=l1(param)), Dense(32, activation='relu', kernel_regularizer=l1(param)), Dense(1, activation='sigmoid'),]) # described sequentially, layer-by-layer with 32, 32, and 1 neurons; 58 input features\n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "    history = model.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_val, Y_val))\n",
    "\n",
    "    # Plot epoch vs. accuracy plot\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    plt.plot(history.history['acc'], linewidth=3.5)\n",
    "    plt.plot(history.history['val_acc'], linewidth=3.5)\n",
    "    plt.plot(history.history['loss'], linewidth=3.5)\n",
    "    plt.plot(history.history['val_loss'], linewidth=3.5)\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.title('SGD Model Training Loss and Accuracy')\n",
    "    plt.ylabel('Accuracy/Loss', fontsize=20)\n",
    "    plt.xlabel('Epoch', fontsize=20)\n",
    "    plt.ylim(0,4)\n",
    "    plt.legend(['Acc_Train', 'Acc_Val', 'Loss_Train', 'Loss_Val'], loc='upper right')\n",
    "    plt.savefig('SGD.png')\n",
    "    print(\"SGD Accuracy: \" + str(model.evaluate(X_test, Y_test)[1]))\n",
    "\n",
    "    param = 0.5\n",
    "    # Adam optimizer\n",
    "    model_2 = Sequential([Dense(100, activation='relu', input_shape=(58,), kernel_regularizer=l1(param)), Dense(100, activation='relu'), Dense(100, activation='relu'), Dense(100, activation='relu'), Dense(1, activation='sigmoid'),])\n",
    "    model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history2 = model_2.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_val, Y_val))\n",
    "\n",
    "    # Plot epoch vs. accuracy plot\n",
    "    plt.figure()\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    plt.plot(history2.history['acc'], linewidth=3.5)\n",
    "    plt.plot(history2.history['val_acc'], linewidth=3.5)\n",
    "    plt.plot(history2.history['loss'], linewidth=3.5)\n",
    "    plt.plot(history2.history['val_loss'], linewidth=3.5)\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    # plt.figure(figsize=(14,10))\n",
    "    plt.title('Adam Model Training Loss and Accuracy')\n",
    "    plt.ylabel('Accuracy/Loss', fontsize=20)\n",
    "    plt.xlabel('Epoch', fontsize=20)\n",
    "    plt.ylim(0,4)\n",
    "    plt.legend(['Acc_Train', 'Acc_Val', 'Loss_Train', 'Loss_Val'], loc='upper right')\n",
    "    plt.savefig('Adam.png')\n",
    "    print(\"Adam Accuracy: \" + str(model_2.evaluate(X_test, Y_test)[1]))\n",
    "\n",
    "    print(\"Success!\")\n",
    "\n",
    "def create_model(optimizer='sgd', init='glorot_uniform', loss = 'binary_crossentropy'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(58,), init=init))\n",
    "    model.add(Dense(32, activation='relu', init=init))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def hyperParamTuning(X,y, test_size):\n",
    "    start=time.time()\n",
    "    model = KerasClassifier(build_fn=create_model)\n",
    "    optimizers = ['rmsprop', 'adam', 'sgd']\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    #'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'\n",
    "    loss = ['categorical_crossentropy', 'binary_crossentropy']\n",
    "    # epochs = np.array([50, 100, 150])\n",
    "    epochs = np.array([50])\n",
    "    batches = np.array([5, 10, 20])\n",
    "    param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "    grid_result = grid.fit(X_train, Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    end = time.time()\n",
    "    print(\"Time\", end-start)\n",
    "\n",
    " #     history = model.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_val, Y_val))\n",
    "\n",
    "    #     # Plot epoch vs. accuracy plot\n",
    "    #     plt.plot(history.history['acc'])\n",
    "    #     plt.plot(history.history['val_acc'])\n",
    "    #     plt.title('Model accuracy')\n",
    "    #     plt.ylabel('Accuracy')\n",
    "    #     plt.xlabel('Epoch')\n",
    "    #     plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    #     plt.show()\n",
    "\n",
    "    #     # Plot epoch vs. loss plot\n",
    "    #     plt.plot(history.history['loss'])\n",
    "    #     plt.plot(history.history['val_loss'])\n",
    "    #     plt.title('Model loss')\n",
    "    #     plt.ylabel('Loss')\n",
    "    #     plt.xlabel('Epoch')\n",
    "    #     plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    #     plt.show()\n",
    "\n",
    "    #     print(\"Accuracy: \" + str(model.evaluate(X_test, Y_test)[1]))\n",
    "    # print(grid_result.grid_scores_)\n",
    "    results = pd.DataFrame.from_dict(grid_result.cv_results_)\n",
    "    print(results)\n",
    "    results.to_csv('results.csv', index=False)\n",
    "    # print(grid_result.cv_results_)\n",
    "\n",
    "    # for params, mean_score, scores in grid_result.cv_results_:\n",
    "    #     print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))\n",
    "    # print(\"total time:\",time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPwYnJxoQBnO"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    matplotlib.rcParams.update({'font.size': 10})\n",
    "    who = 'supervisor'\n",
    "    if who != 'supervisor' and who != 'coworkers':\n",
    "        print('Incorrect Inputs')\n",
    "    train, combined = loadData(who)\n",
    "    #weights_supervisor_yes\n",
    "    X,y,feature_cols = buildData(combined, who, train)\n",
    "    logreg = performLogReg(X,y)\n",
    "    weights = printAndShowCoef(feature_cols, logreg)\n",
    "    printListCoefs(X, logreg)\n",
    "    runDifferentModels(X,y)\n",
    "    #     runKNN_SVMModels(X,Y)\n",
    "    testPCA(X, y)\n",
    "    runNeuralNet(X, y, 0.3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS 221 Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
